{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\samue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\samue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import product\n",
    "import json\n",
    "\n",
    "\n",
    "# Inicializa√ß√µes do NLTK\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©-processamento de texto\n",
    "preprocess_methods = {\n",
    "    'raw': lambda x: x,\n",
    "    'clean': lambda text: re.sub(r'[^\\w\\s]', '', re.sub(r'\\d+', '', re.sub(r'[^\\x00-\\x7F]+', '', text.lower()))),\n",
    "    'stopwords': lambda text: ' '.join([word for word in text.split() if word not in stop_words]),\n",
    "    'lemmatization': lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split()]),\n",
    "    'stopwords_lemmatization': lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]),\n",
    "\n",
    "    'clean_stopwords': lambda text: ' '.join(\n",
    "        [word for word in re.sub(r'[^\\w\\s]', '', \n",
    "        re.sub(r'\\d+', '', \n",
    "        re.sub(r'[^\\x00-\\x7F]+', '', text.lower()))).split() if word not in stop_words]\n",
    "    ),\n",
    "\n",
    "    'clean_lemmatization': lambda text: ' '.join(\n",
    "        [lemmatizer.lemmatize(word) for word in re.sub(r'[^\\w\\s]', '', \n",
    "        re.sub(r'\\d+', '', \n",
    "        re.sub(r'[^\\x00-\\x7F]+', '', text.lower()))).split()]\n",
    "    ),\n",
    "\n",
    "    'clean_stopwords_lemmatization': lambda text: ' '.join(\n",
    "        [lemmatizer.lemmatize(word) for word in re.sub(r'[^\\w\\s]', '', \n",
    "        re.sub(r'\\d+', '', \n",
    "        re.sub(r'[^\\x00-\\x7F]+', '', text.lower()))).split() if word not in stop_words]\n",
    "    )\n",
    "}\n",
    "\n",
    "# Gerar embeddings\n",
    "def get_text_features(texts, model_name):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Fun√ß√£o para salvar a matriz de confus√£o como imagem\n",
    "def save_confusion_matrix(cm, class_names, output_path, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(class_names)), class_names, rotation=45, ha='right')\n",
    "    plt.yticks(np.arange(len(class_names)), class_names)\n",
    "\n",
    "    threshold = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > threshold else \"black\")\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Predicted\", fontsize=12)\n",
    "    plt.ylabel(\"True Labels\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "# Salvar relat√≥rio de classifica√ß√£o em CSV\n",
    "def save_classification_metrics(y_true, y_pred, class_names, output_path):\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    df_report.to_csv(output_path, index=True)\n",
    "\n",
    "# Treinar KMeans com inicializa√ß√£o pelos centr√≥ides m√©dios de cada classe\n",
    "def train_kmeans(train_data, init_centroids):\n",
    "    kmeans_instance = KMeans(n_clusters=len(init_centroids), n_init=1, init=init_centroids, random_state=42)\n",
    "    kmeans_instance.fit(train_data)\n",
    "    return kmeans_instance\n",
    "\n",
    "# Testar o modelo KMeans\n",
    "def test_kmeans(kmeans, test_data):\n",
    "    return kmeans.predict(test_data)\n",
    "\n",
    "def run_kmeans_cv(X, y, k_folds, seed, output_base_dir, le):\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "    y_true_general, y_pred_general = [], []\n",
    "    results = {}\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    for fold_idx, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        #init_centroids = np.array([X_train[y_train == i].mean(axis=0) for i in np.unique(y_train)])\n",
    "        init_centroids = np.array(pd.DataFrame(X_train).groupby(y_train).mean())\n",
    "        kmeans_instance = train_kmeans(X_train, init_centroids)\n",
    "        y_pred = test_kmeans(kmeans_instance, X_test)\n",
    "        \n",
    "        fold_dir = os.path.join(output_base_dir, f'{k_folds}_folds/fold_{fold_idx + 1}')\n",
    "        os.makedirs(fold_dir, exist_ok=True)    \n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "        save_confusion_matrix(cm, le.classes_, os.path.join(fold_dir, 'confusion_matrix.png'), f\"Fold {fold_idx + 1}\")\n",
    "        save_classification_metrics(y_test, y_pred, le.classes_, os.path.join(fold_dir, 'classification_metrics.csv'))\n",
    "\n",
    "        results[f'fold_{fold_idx + 1}'] =  accuracy_score(y_test, y_pred)\n",
    "        y_true_general.extend(y_test)\n",
    "        y_pred_general.extend(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_true_general, y_pred_general, labels=classes)\n",
    "    save_confusion_matrix(cm, le.classes_, os.path.join(output_base_dir, f'{k_folds}_folds/confusion_matrix.png'), f\"Geral - {k_folds}_folds\")\n",
    "    save_classification_metrics(y_true_general, y_pred_general, le.classes_, os.path.join(output_base_dir, f'{k_folds}_folds/classification_metrics.csv'))\n",
    "\n",
    "    overall_accuracy = accuracy_score(y_true_general, y_pred_general)\n",
    "    results['overall_accuracy'] = overall_accuracy\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏èProcessando: raw | Modelo: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea94c1a08e04350ab047dad795e19c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\raw_paraphrase-multilingual-MiniLM-L12-v2\n",
      "| - 5_folds >>> accuracy: 0.5775075987841946\n",
      "| - 10_folds >>> accuracy: 0.5820668693009119\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\raw_paraphrase-multilingual-MiniLM-L12-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: raw | Modelo: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da4df623503444380505b8fa3a3db08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\raw_all-MiniLM-L6-v2\n",
      "| - 5_folds >>> accuracy: 0.6033434650455927\n",
      "| - 10_folds >>> accuracy: 0.6033434650455927\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\raw_all-MiniLM-L6-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: clean | Modelo: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2beb3ef5b7824c9a933c3f4db740d615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_paraphrase-multilingual-MiniLM-L12-v2\n",
      "| - 5_folds >>> accuracy: 0.6003039513677811\n",
      "| - 10_folds >>> accuracy: 0.5927051671732523\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_paraphrase-multilingual-MiniLM-L12-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: clean | Modelo: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988b4adc2401481daf75450bc5abc22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_all-MiniLM-L6-v2\n",
      "| - 5_folds >>> accuracy: 0.5957446808510638\n",
      "| - 10_folds >>> accuracy: 0.6003039513677811\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_all-MiniLM-L6-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: stopwords | Modelo: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed33f187b1f47559625dca7b5e6b778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\stopwords_paraphrase-multilingual-MiniLM-L12-v2\n",
      "| - 5_folds >>> accuracy: 0.5927051671732523\n",
      "| - 10_folds >>> accuracy: 0.601823708206687\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\stopwords_paraphrase-multilingual-MiniLM-L12-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: stopwords | Modelo: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de4f281a5e14b46beb5465c86518124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\stopwords_all-MiniLM-L6-v2\n",
      "| - 5_folds >>> accuracy: 0.5866261398176292\n",
      "| - 10_folds >>> accuracy: 0.5987841945288754\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\stopwords_all-MiniLM-L6-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: lemmatization | Modelo: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b363f1ab53c407ca5407fc044cda879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\lemmatization_paraphrase-multilingual-MiniLM-L12-v2\n",
      "| - 5_folds >>> accuracy: 0.5668693009118541\n",
      "| - 10_folds >>> accuracy: 0.574468085106383\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\lemmatization_paraphrase-multilingual-MiniLM-L12-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: lemmatization | Modelo: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b926d718b44dd6aa0d1c4f8bbef209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\lemmatization_all-MiniLM-L6-v2\n",
      "| - 5_folds >>> accuracy: 0.5942249240121581\n",
      "| - 10_folds >>> accuracy: 0.5927051671732523\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\lemmatization_all-MiniLM-L6-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: stopwords_lemmatization | Modelo: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d3f998deeb491cb0dc429b66442cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\stopwords_lemmatization_paraphrase-multilingual-MiniLM-L12-v2\n",
      "| - 5_folds >>> accuracy: 0.5896656534954408\n",
      "| - 10_folds >>> accuracy: 0.5820668693009119\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\stopwords_lemmatization_paraphrase-multilingual-MiniLM-L12-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: stopwords_lemmatization | Modelo: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbda9e3e09845bc9ecb0648c9721902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\stopwords_lemmatization_all-MiniLM-L6-v2\n",
      "| - 5_folds >>> accuracy: 0.5927051671732523\n",
      "| - 10_folds >>> accuracy: 0.60790273556231\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\stopwords_lemmatization_all-MiniLM-L6-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: clean_stopwords | Modelo: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f83150822a04512bf56e93ab336a908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_stopwords_paraphrase-multilingual-MiniLM-L12-v2\n",
      "| - 5_folds >>> accuracy: 0.5957446808510638\n",
      "| - 10_folds >>> accuracy: 0.5957446808510638\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_stopwords_paraphrase-multilingual-MiniLM-L12-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: clean_stopwords | Modelo: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35889b09aa5e42ada039dc56e1e97e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_stopwords_all-MiniLM-L6-v2\n",
      "| - 5_folds >>> accuracy: 0.5775075987841946\n",
      "| - 10_folds >>> accuracy: 0.5790273556231003\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_stopwords_all-MiniLM-L6-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: clean_lemmatization | Modelo: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee43b38efcb414db7c3b856b1d2758d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_lemmatization_paraphrase-multilingual-MiniLM-L12-v2\n",
      "| - 5_folds >>> accuracy: 0.5987841945288754\n",
      "| - 10_folds >>> accuracy: 0.5896656534954408\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_lemmatization_paraphrase-multilingual-MiniLM-L12-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: clean_lemmatization | Modelo: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2812293751649f8bec89bcbe313dcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_lemmatization_all-MiniLM-L6-v2\n",
      "| - 5_folds >>> accuracy: 0.6048632218844985\n",
      "| - 10_folds >>> accuracy: 0.5987841945288754\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_lemmatization_all-MiniLM-L6-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: clean_stopwords_lemmatization | Modelo: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a621183f6cb64c109184a39b1875064c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_stopwords_lemmatization_paraphrase-multilingual-MiniLM-L12-v2\n",
      "| - 5_folds >>> accuracy: 0.5927051671732523\n",
      "| - 10_folds >>> accuracy: 0.5881458966565349\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_stopwords_lemmatization_paraphrase-multilingual-MiniLM-L12-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n",
      "‚ö†Ô∏èProcessando: clean_stopwords_lemmatization | Modelo: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c888f62adfb7442dbbfd498c158f7122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMatriz embeddings Salva! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_stopwords_lemmatization_all-MiniLM-L6-v2\n",
      "| - 5_folds >>> accuracy: 0.5835866261398176\n",
      "| - 10_folds >>> accuracy: 0.5790273556231003\n",
      "‚úÖDicion√°rio salvo em C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\\clean_stopwords_lemmatization_all-MiniLM-L6-v2/metrics.json\n",
      "‚úÖProcessamento conclu√≠do!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_csv_path      = 'C:/Users/samue/Downloads/dataset_v2_atributos_25.csv'\n",
    "data = pd.read_csv(dataset_csv_path, header=0, sep=';')\n",
    "data['category'] = data['category'].str.replace('_', '-')\n",
    "data = data[data['category'].notnull()].reset_index(drop=True)\n",
    "data = data[~data['category'].isin(['domain-names', 'sports-collectibles'])]\n",
    "\n",
    "# Filtrar data2 para manter apenas as linhas cujas 'slug' est√£o presentes em data\n",
    "slugs_data = set(data['slug'])\n",
    "\n",
    "# Principal\n",
    "if __name__ == \"__main__\":\n",
    "    #data = pd.read_csv(\"C:/Users/samue/Downloads/df_with_description.csv\")\n",
    "    data = pd.read_csv(\"C:/Users/samue/Downloads/dataset_description.csv\")\n",
    "    #data = pd.read_csv(\"C:/Users/samue/Downloads/dataset_description_25.csv\")\n",
    "    data = data[~data['category'].isin(['domain-names', 'sports-collectibles'])]\n",
    "    data = data[data['slug'] != 'panoramic-portraits'].reset_index(drop=True)\n",
    "    #data = data.groupby('category').head(25).reset_index(drop=True)\n",
    "\n",
    "    data_filtrado = data[data['slug'].isin(slugs_data)].reset_index(drop=True)\n",
    "    data = data_filtrado.copy()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(data['category'])\n",
    "\n",
    "    embedding_models = [\n",
    "        \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"all-MiniLM-L6-v2\"\n",
    "    ]\n",
    "\n",
    "    #output_base = \"C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys\"\n",
    "    output_base = \"C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys_set\"\n",
    "\n",
    "    k_folds = [5, 10] \n",
    "    seed = 42\n",
    "    results = {}\n",
    "    for (prep_name, prep_func), model_name in product(preprocess_methods.items(), embedding_models):\n",
    "\n",
    "        dir_name = f\"{prep_name}_{model_name.replace('/', '_')}\"\n",
    "        output_dir = os.path.join(output_base, dir_name)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"‚ö†Ô∏èProcessando: {prep_name} | Modelo: {model_name}\")\n",
    "        processed_text = data['description'].fillna('').apply(prep_func)\n",
    "        embeddings = get_text_features(processed_text, model_name)\n",
    "        \n",
    "        np.save(os.path.join(output_dir, 'embeddings.npy'), embeddings)\n",
    "        print(f'‚úÖMatriz embeddings Salva! >>> {output_dir}')\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(embeddings)\n",
    "        for fold in k_folds:\n",
    "            results[str(fold) + '_folds'] = run_kmeans_cv(X_scaled, y, fold, seed, output_dir, label_encoder)\n",
    "            print(f'| - {fold}_folds >>> accuracy: {results[str(fold) + \"_folds\"][\"overall_accuracy\"]}')\n",
    "\n",
    "        nome_arquivo = f'{output_dir}/metrics.json'\n",
    "        with open(nome_arquivo, 'w') as arquivo_json:\n",
    "            json.dump(results, arquivo_json, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"‚úÖDicion√°rio salvo em {nome_arquivo}\")\n",
    "        print(f\"‚úÖProcessamento conclu√≠do!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zname</th>\n",
       "      <th>slug</th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bored Ape Yacht Club</td>\n",
       "      <td>boredapeyachtclub</td>\n",
       "      <td>pfps</td>\n",
       "      <td>The Bored Ape Yacht Club is a collection of 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CryptoPunks</td>\n",
       "      <td>cryptopunks</td>\n",
       "      <td>pfps</td>\n",
       "      <td>CryptoPunks launched as a fixed set of 10,000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mutant Ape Yacht Club</td>\n",
       "      <td>mutant-ape-yacht-club</td>\n",
       "      <td>pfps</td>\n",
       "      <td>The MUTANT APE YACHT CLUB is a collection of u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Azuki</td>\n",
       "      <td>azuki</td>\n",
       "      <td>pfps</td>\n",
       "      <td>Take the red bean to join the garden. View the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CLONE X - X TAKASHI MURAKAMI</td>\n",
       "      <td>clonex</td>\n",
       "      <td>pfps</td>\n",
       "      <td>üß¨ CLONE X üß¨\\n\\n20,000 next-gen Avatars, by RTF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>Eric Rubens Editions</td>\n",
       "      <td>eric-rubens-editions</td>\n",
       "      <td>photography</td>\n",
       "      <td>Eric Rubens editions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>Skateboarding: Culture in Motion</td>\n",
       "      <td>scim</td>\n",
       "      <td>photography</td>\n",
       "      <td>‚ÄúSkateboarding: Culture in Motion‚Äù is a collec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>Afromythology by Shawn Theodore</td>\n",
       "      <td>afromythology-by-shawn-theodore</td>\n",
       "      <td>photography</td>\n",
       "      <td>Bucking traditional photographic formalism, Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>NYC Underground Stories by Monaris</td>\n",
       "      <td>nycus</td>\n",
       "      <td>photography</td>\n",
       "      <td>Picture the New York Subway and the millions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>Deadfrenz Lab Access Pass</td>\n",
       "      <td>deadfrenz-lab-access-pass</td>\n",
       "      <td>memberships</td>\n",
       "      <td>Created by [Deadfellaz](https://opensea.io/col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>658 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Zname                             slug  \\\n",
       "0                  Bored Ape Yacht Club                boredapeyachtclub   \n",
       "1                           CryptoPunks                      cryptopunks   \n",
       "2                 Mutant Ape Yacht Club            mutant-ape-yacht-club   \n",
       "3                                 Azuki                            azuki   \n",
       "4          CLONE X - X TAKASHI MURAKAMI                           clonex   \n",
       "..                                  ...                              ...   \n",
       "653                Eric Rubens Editions             eric-rubens-editions   \n",
       "654    Skateboarding: Culture in Motion                             scim   \n",
       "655     Afromythology by Shawn Theodore  afromythology-by-shawn-theodore   \n",
       "656  NYC Underground Stories by Monaris                            nycus   \n",
       "657           Deadfrenz Lab Access Pass        deadfrenz-lab-access-pass   \n",
       "\n",
       "        category                                        description  \n",
       "0           pfps  The Bored Ape Yacht Club is a collection of 10...  \n",
       "1           pfps  CryptoPunks launched as a fixed set of 10,000 ...  \n",
       "2           pfps  The MUTANT APE YACHT CLUB is a collection of u...  \n",
       "3           pfps  Take the red bean to join the garden. View the...  \n",
       "4           pfps  üß¨ CLONE X üß¨\\n\\n20,000 next-gen Avatars, by RTF...  \n",
       "..           ...                                                ...  \n",
       "653  photography                               Eric Rubens editions  \n",
       "654  photography  ‚ÄúSkateboarding: Culture in Motion‚Äù is a collec...  \n",
       "655  photography  Bucking traditional photographic formalism, Sh...  \n",
       "656  photography  Picture the New York Subway and the millions o...  \n",
       "657  memberships  Created by [Deadfellaz](https://opensea.io/col...  \n",
       "\n",
       "[658 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelos</th>\n",
       "      <th>preprocessamento</th>\n",
       "      <th>kfolds</th>\n",
       "      <th>acuracia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.594993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.593520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>stopwords_lemmatization</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.593520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>stopwords_lemmatization</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.592047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>raw</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.592047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.589102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.589102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>clean_lemmatization</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.586156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>stopwords_lemmatization</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.584683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>clean_stopwords</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.584683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>clean_stopwords</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.581738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>raw</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.581738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>clean_lemmatization</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.581738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>lemmatization</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.578792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>clean</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.578792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>raw</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.577320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>clean</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.577320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>clean</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.577320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>lemmatization</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.575847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>lemmatization</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.575847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>clean_lemmatization</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.574374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>clean_stopwords_lemmatization</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.572901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>clean_lemmatization</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.572901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>clean_stopwords_lemmatization</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.569956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>lemmatization</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.568483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>clean</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.568483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>stopwords_lemmatization</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.567010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>clean_stopwords_lemmatization</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.567010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>raw</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.565538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>clean_stopwords</td>\n",
       "      <td>5_folds</td>\n",
       "      <td>0.562592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>clean_stopwords_lemmatization</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.559647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>clean_stopwords</td>\n",
       "      <td>10_folds</td>\n",
       "      <td>0.559647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  modelos               preprocessamento  \\\n",
       "24                       all-MiniLM-L6-v2                      stopwords   \n",
       "31  paraphrase-multilingual-MiniLM-L12-v2                      stopwords   \n",
       "26                       all-MiniLM-L6-v2        stopwords_lemmatization   \n",
       "27                       all-MiniLM-L6-v2        stopwords_lemmatization   \n",
       "20                       all-MiniLM-L6-v2                            raw   \n",
       "30  paraphrase-multilingual-MiniLM-L12-v2                      stopwords   \n",
       "25                       all-MiniLM-L6-v2                      stopwords   \n",
       "2                        all-MiniLM-L6-v2            clean_lemmatization   \n",
       "29  paraphrase-multilingual-MiniLM-L12-v2        stopwords_lemmatization   \n",
       "15  paraphrase-multilingual-MiniLM-L12-v2                clean_stopwords   \n",
       "14  paraphrase-multilingual-MiniLM-L12-v2                clean_stopwords   \n",
       "21                       all-MiniLM-L6-v2                            raw   \n",
       "3                        all-MiniLM-L6-v2            clean_lemmatization   \n",
       "16                       all-MiniLM-L6-v2                  lemmatization   \n",
       "1                        all-MiniLM-L6-v2                          clean   \n",
       "22  paraphrase-multilingual-MiniLM-L12-v2                            raw   \n",
       "7   paraphrase-multilingual-MiniLM-L12-v2                          clean   \n",
       "0                        all-MiniLM-L6-v2                          clean   \n",
       "18  paraphrase-multilingual-MiniLM-L12-v2                  lemmatization   \n",
       "17                       all-MiniLM-L6-v2                  lemmatization   \n",
       "5   paraphrase-multilingual-MiniLM-L12-v2            clean_lemmatization   \n",
       "13  paraphrase-multilingual-MiniLM-L12-v2  clean_stopwords_lemmatization   \n",
       "4   paraphrase-multilingual-MiniLM-L12-v2            clean_lemmatization   \n",
       "12  paraphrase-multilingual-MiniLM-L12-v2  clean_stopwords_lemmatization   \n",
       "19  paraphrase-multilingual-MiniLM-L12-v2                  lemmatization   \n",
       "6   paraphrase-multilingual-MiniLM-L12-v2                          clean   \n",
       "28  paraphrase-multilingual-MiniLM-L12-v2        stopwords_lemmatization   \n",
       "10                       all-MiniLM-L6-v2  clean_stopwords_lemmatization   \n",
       "23  paraphrase-multilingual-MiniLM-L12-v2                            raw   \n",
       "8                        all-MiniLM-L6-v2                clean_stopwords   \n",
       "11                       all-MiniLM-L6-v2  clean_stopwords_lemmatization   \n",
       "9                        all-MiniLM-L6-v2                clean_stopwords   \n",
       "\n",
       "      kfolds  acuracia  \n",
       "24   5_folds  0.594993  \n",
       "31  10_folds  0.593520  \n",
       "26   5_folds  0.593520  \n",
       "27  10_folds  0.592047  \n",
       "20   5_folds  0.592047  \n",
       "30   5_folds  0.589102  \n",
       "25  10_folds  0.589102  \n",
       "2    5_folds  0.586156  \n",
       "29  10_folds  0.584683  \n",
       "15  10_folds  0.584683  \n",
       "14   5_folds  0.581738  \n",
       "21  10_folds  0.581738  \n",
       "3   10_folds  0.581738  \n",
       "16   5_folds  0.578792  \n",
       "1   10_folds  0.578792  \n",
       "22   5_folds  0.577320  \n",
       "7   10_folds  0.577320  \n",
       "0    5_folds  0.577320  \n",
       "18   5_folds  0.575847  \n",
       "17  10_folds  0.575847  \n",
       "5   10_folds  0.574374  \n",
       "13  10_folds  0.572901  \n",
       "4    5_folds  0.572901  \n",
       "12   5_folds  0.569956  \n",
       "19  10_folds  0.568483  \n",
       "6    5_folds  0.568483  \n",
       "28   5_folds  0.567010  \n",
       "10   5_folds  0.567010  \n",
       "23  10_folds  0.565538  \n",
       "8    5_folds  0.562592  \n",
       "11  10_folds  0.559647  \n",
       "9   10_folds  0.559647  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho base onde est√£o as pastas\n",
    "base_path = \"C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys\"\n",
    "\n",
    "# Listas dos tipos de pr√©-processamento e modelos\n",
    "preprocess_methods = [\n",
    "    'clean_stopwords_lemmatization', 'clean_stopwords', 'clean_lemmatization',\n",
    "    'stopwords_lemmatization', 'clean', 'stopwords', 'lemmatization', 'raw']\n",
    "    \n",
    "\n",
    "embedding_models = [\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"all-MiniLM-L6-v2\"\n",
    "]\n",
    "\n",
    "# Lista para armazenar os resultados\n",
    "results = []\n",
    "\n",
    "# Iterar pelas pastas dentro do diret√≥rio base\n",
    "for folder in os.listdir(base_path):\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    metrics_file = os.path.join(folder_path, 'metrics.json')\n",
    "    \n",
    "    if os.path.isdir(folder_path) and os.path.exists(metrics_file):\n",
    "        # Identificar o tipo de pr√©-processamento e o modelo\n",
    "        preprocess = next((p for p in preprocess_methods if p in folder), 'Desconhecido')\n",
    "        model = next((m for m in embedding_models if m in folder), 'Desconhecido')\n",
    "        \n",
    "        # Ler o arquivo metrics.json\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Adicionar cada resultado ao DataFrame\n",
    "        for kfold, values in data.items():\n",
    "            if 'overall_accuracy' in values:\n",
    "                results.append({\n",
    "                    'modelos': model,\n",
    "                    'preprocessamento': preprocess,\n",
    "                    'kfolds': kfold,\n",
    "                    'acuracia': values['overall_accuracy']\n",
    "                })\n",
    "\n",
    "# Criar DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.sort_values(by=['acuracia', 'modelos', 'preprocessamento', 'kfolds'], ascending=False, inplace=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resultados organizados e salvos com sucesso em: C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys/results_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Salvar o resultado em um arquivo CSV\n",
    "output_path = \"C:/Users/samue/Downloads/NFT25/results_dynamic_7categorys/results_summary.csv\"\n",
    "df_results.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Resultados organizados e salvos com sucesso em: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C√≥dido Dayan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando KMeans com 3 folds...\n",
      "Executando KMeans com 5 folds...\n",
      "Executando KMeans com 10 folds...\n",
      "Processamento conclu√≠do!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_dirs(base_dir, sub_dirs):\n",
    "    for sub_dir in sub_dirs:\n",
    "        os.makedirs(os.path.join(base_dir, sub_dir), exist_ok=True)\n",
    "\n",
    "def save_confusion_matrix(cm, class_names, output_path, title):\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def save_classification_metrics(y_true, y_pred, class_names, output_path):\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    df_report.to_csv(output_path, index=True)\n",
    "\n",
    "def train_kmeans(train_data, init_centroids):\n",
    "    kmeans_instance = KMeans(n_clusters=9, n_init=1, init=init_centroids, random_state=42)\n",
    "    kmeans_instance.fit(train_data)\n",
    "    return kmeans_instance\n",
    "\n",
    "def test_kmeans(kmeans, test_data):\n",
    "    return kmeans.predict(test_data)\n",
    "\n",
    "def run_kmeans_cv(X, y, k_folds, seed, output_base_dir, fold_col_suffix):\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "    classes = np.unique(y)\n",
    "    y_true_general, y_pred_general = [], []\n",
    "\n",
    "    for fold_idx, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Treinar o KMeans\n",
    "        init_centroids = np.array(pd.DataFrame(X_train).groupby(y_train).mean())\n",
    "        kmeans_instance = train_kmeans(X_train, init_centroids)\n",
    "        y_pred = test_kmeans(kmeans_instance, X_test)\n",
    "\n",
    "        y_true_general.extend(y_test)\n",
    "        y_pred_general.extend(y_pred)\n",
    "\n",
    "        # Salvar matriz de confus√£o por fold\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "        fold_dir = os.path.join(output_base_dir, f'fold_{fold_idx + 1}')\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        save_confusion_matrix(cm, le.classes_, os.path.join(fold_dir, 'confusion_matrix.png'), f\"Fold {fold_idx + 1}\")\n",
    "\n",
    "        # Salvar m√©tricas de classifica√ß√£o\n",
    "        save_classification_metrics(y_test, y_pred, le.classes_, os.path.join(fold_dir, 'classification_metrics.csv'))\n",
    "\n",
    "    # Matriz de confus√£o geral\n",
    "    cm_general = confusion_matrix(y_true_general, y_pred_general, labels=classes)\n",
    "    save_confusion_matrix(cm_general, le.classes_, os.path.join(output_base_dir, 'general_confusion_matrix.png'), \"Geral\")\n",
    "    save_classification_metrics(y_true_general, y_pred_general, le.classes_, os.path.join(output_base_dir, 'general_classification_metrics.csv'))\n",
    "\n",
    "# Carregar os dados\n",
    "columns = ['total_volume', 'total_sales', 'total_supply', 'num_owners', 'average_price', 'market_cap', 'qtd_traits', 'qtd_editors', 'category']\n",
    "df_path_710 = 'C:/Users/samue/Downloads/dataset_v2_atributos.csv'\n",
    "data_710 = pd.read_csv(df_path_710)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_710[columns[:-1]])\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data_710['category'])\n",
    "\n",
    "# Definir sa√≠da\n",
    "output_base_dir = \"C:/Users/samue/Downloads/NFT25/dayan\"\n",
    "create_dirs(output_base_dir, ['3_folds', '5_folds', '10_folds'])\n",
    "\n",
    "# Executar KMeans com 3, 5 e 10 folds\n",
    "print(\"Executando KMeans com 3 folds...\")\n",
    "run_kmeans_cv(data_scaled, y, 3, 42, os.path.join(output_base_dir, '3_folds'), '3fold')\n",
    "\n",
    "print(\"Executando KMeans com 5 folds...\")\n",
    "run_kmeans_cv(data_scaled, y, 5, 42, os.path.join(output_base_dir, '5_folds'), '5fold')\n",
    "\n",
    "print(\"Executando KMeans com 10 folds...\")\n",
    "run_kmeans_cv(data_scaled, y, 10, 42, os.path.join(output_base_dir, '10_folds'), '10fold')\n",
    "\n",
    "print(\"Processamento conclu√≠do!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling simples e smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©-processamento de texto\n",
    "preprocess_methods = {\n",
    "    'raw': lambda x: x,\n",
    "    'clean': lambda text: re.sub(r'[^\\w\\s]', '', re.sub(r'\\d+', '', re.sub(r'[^\\x00-\\x7F]+', '', text.lower()))),\n",
    "    'stopwords': lambda text: ' '.join([word for word in text.split() if word not in stop_words]),\n",
    "    'lemmatization': lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split()]),\n",
    "    'stopwords_lemmatization': lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]),\n",
    "\n",
    "    'clean_stopwords': lambda text: ' '.join(\n",
    "        [word for word in re.sub(r'[^\\w\\s]', '', \n",
    "        re.sub(r'\\d+', '', \n",
    "        re.sub(r'[^\\x00-\\x7F]+', '', text.lower()))).split() if word not in stop_words]\n",
    "    ),\n",
    "\n",
    "    'clean_lemmatization': lambda text: ' '.join(\n",
    "        [lemmatizer.lemmatize(word) for word in re.sub(r'[^\\w\\s]', '', \n",
    "        re.sub(r'\\d+', '', \n",
    "        re.sub(r'[^\\x00-\\x7F]+', '', text.lower()))).split()]\n",
    "    ),\n",
    "\n",
    "    'clean_stopwords_lemmatization': lambda text: ' '.join(\n",
    "        [lemmatizer.lemmatize(word) for word in re.sub(r'[^\\w\\s]', '', \n",
    "        re.sub(r'\\d+', '', \n",
    "        re.sub(r'[^\\x00-\\x7F]+', '', text.lower()))).split() if word not in stop_words]\n",
    "    )\n",
    "}\n",
    "\n",
    "# Gerar embeddings\n",
    "def get_text_features(texts, model_name):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Fun√ß√£o para salvar a matriz de confus√£o como imagem\n",
    "def save_confusion_matrix(cm, class_names, output_path, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(class_names)), class_names, rotation=45, ha='right')\n",
    "    plt.yticks(np.arange(len(class_names)), class_names)\n",
    "\n",
    "    threshold = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > threshold else \"black\")\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Predicted\", fontsize=12)\n",
    "    plt.ylabel(\"True Labels\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "# Salvar relat√≥rio de classifica√ß√£o em CSV\n",
    "def save_classification_metrics(y_true, y_pred, class_names, output_path):\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    df_report.to_csv(output_path, index=True)\n",
    "\n",
    "# Treinar KMeans com inicializa√ß√£o pelos centr√≥ides m√©dios de cada classe\n",
    "def train_kmeans(train_data, init_centroids):\n",
    "    kmeans_instance = KMeans(n_clusters=len(init_centroids), n_init=1, init=init_centroids, random_state=42)\n",
    "    kmeans_instance.fit(train_data)\n",
    "    return kmeans_instance\n",
    "\n",
    "# Testar o modelo KMeans\n",
    "def test_kmeans(kmeans, test_data):\n",
    "    return kmeans.predict(test_data)\n",
    "\n",
    "# Fun√ß√£o para gerar o gr√°fico\n",
    "def plot_kmeans_clusters(X, kmeans):\n",
    "    # Predi√ß√£o do KMeans nos dados\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Contagem de dados por cluster\n",
    "    cluster_counts = np.bincount(labels)\n",
    "    \n",
    "    # Plotando gr√°fico de barras\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=np.arange(len(cluster_counts)), y=cluster_counts, palette='viridis')\n",
    "    plt.title('N√∫mero de Dados por Cluster', fontsize=16)\n",
    "    plt.xlabel('Cluster', fontsize=12)\n",
    "    plt.ylabel('N√∫mero de Dados', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# Plotar gr√°fico de dispers√£o\n",
    "def plot_kmeans_scatter(X, kmeans):\n",
    "    # Predi√ß√£o do KMeans nos dados\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "    plt.title('Visualiza√ß√£o dos Clusters KMeans', fontsize=16)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.show()\n",
    "    \n",
    "def run_kmeans_cv(X, y, k_folds, seed, output_base_dir, le, smote):\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "    y_true_general, y_pred_general = [], []\n",
    "    results = {}\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    for fold_idx, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Passo 1: Treinar o KMeans nos dados de treino\n",
    "        kmeans_initial = KMeans(n_clusters=3, random_state=seed)\n",
    "        kmeans_initial.fit(X_train)\n",
    "        cluster_labels = kmeans_initial.predict(X_train)\n",
    "\n",
    "        # Plotar a quantidade de dados por cluster\n",
    "        #plot_kmeans_clusters(X_train, kmeans_initial)\n",
    "        \n",
    "        # Plotar a visualiza√ß√£o dos clusters\n",
    "        #plot_kmeans_scatter(X_train, kmeans_initial)\n",
    "\n",
    "        # Passo 2: Filtrar os melhores clusters\n",
    "        # Vamos filtrar os clusters com base na densidade ou na quantidade de pontos em cada cluster\n",
    "        cluster_counts = np.bincount(cluster_labels)\n",
    "        \n",
    "        # Encontrar o √≠ndice do cluster com o menor n√∫mero de elementos\n",
    "        smallest_cluster = np.argmin(cluster_counts)    \n",
    "        #filtered_clusters = np.where(cluster_counts > 10)[0]  # Manter apenas os clusters com mais de 10 pontos\n",
    "\n",
    "        # Excluir o cluster com o menor n√∫mero de elementos\n",
    "        filter_mask = cluster_labels != smallest_cluster\n",
    "        # Filtrar os dados de treino com base nos clusters selecionados\n",
    "        #filter_mask = np.isin(cluster_labels, filtered_clusters)\n",
    "        \n",
    "        X_train_filtered = X_train[filter_mask]\n",
    "        y_train_filtered = y_train[filter_mask]\n",
    "\n",
    "        X_train = X_train_filtered\n",
    "        y_train = y_train_filtered\n",
    "        # Aplicar SMOTE apenas no conjunto de treinamento\n",
    "        if smote:\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Treinar KMeans com dados balanceados\n",
    "        init_centroids = np.array(pd.DataFrame(X_train).groupby(y_train).mean())\n",
    "        kmeans_instance = train_kmeans(X_train, init_centroids)\n",
    "        y_pred = test_kmeans(kmeans_instance, X_test)\n",
    "\n",
    "        fold_dir = os.path.join(output_base_dir, f'{k_folds}_folds/fold_{fold_idx + 1}')\n",
    "        os.makedirs(fold_dir, exist_ok=True)    \n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "        save_confusion_matrix(cm, le.classes_, os.path.join(fold_dir, 'confusion_matrix.png'), f\"Fold {fold_idx + 1}\")\n",
    "        save_classification_metrics(y_test, y_pred, le.classes_, os.path.join(fold_dir, 'classification_metrics.csv'))\n",
    "    \n",
    "        results[f'fold_{fold_idx + 1}'] =  accuracy_score(y_test, y_pred)\n",
    "        y_true_general.extend(y_test)\n",
    "        y_pred_general.extend(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_true_general, y_pred_general, labels=classes)\n",
    "    save_confusion_matrix(cm, le.classes_, os.path.join(output_base_dir, f'{k_folds}_folds/confusion_matrix.png'), f\"Geral - {k_folds}_folds\")\n",
    "    save_classification_metrics(y_true_general, y_pred_general, le.classes_, os.path.join(output_base_dir, f'{k_folds}_folds/classification_metrics.csv'))\n",
    "\n",
    "    overall_accuracy = accuracy_score(y_true_general, y_pred_general)\n",
    "    results['overall_accuracy'] = overall_accuracy\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏èProcessando: raw | Modelo: paraphrase-multilingual-MiniLM-L12-v2\n",
      "‚úÖMatriz embeddings carregada! >>> C:/Users/samue/Downloads/NFT25/results_dynamic_2\\raw_paraphrase-multilingual-MiniLM-L12-v2\\embeddings.npy\n",
      "‚úÖMatriz embeddings >>> (711, 384)\n",
      "‚úÖMatriz embeddings >>> [[ 0.04778653  0.03305385  0.12186813 ...  0.09785338 -0.30351835\n",
      "   0.14238296]\n",
      " [-0.2104857  -0.26380208 -0.21774286 ... -0.20872392 -0.04061646\n",
      "   0.11463138]\n",
      " [-0.19266498 -0.07589609 -0.19777267 ...  0.08735441  0.17186333\n",
      "   0.14984657]\n",
      " ...\n",
      " [ 0.09293503  0.14670089 -0.25660846 ...  0.20481679  0.00595592\n",
      "  -0.02706998]\n",
      " [ 0.14858088 -0.1650946   0.15047547 ... -0.04498821 -0.40291464\n",
      "  -0.10871606]\n",
      " [-0.17094752  0.06419861 -0.2678259  ... -0.04350909  0.10477987\n",
      "  -0.03905095]]\n"
     ]
    }
   ],
   "source": [
    "# Principal\n",
    "if __name__ == \"__main__\":\n",
    "    #data = pd.read_csv(\"C:/Users/samue/Downloads/df_with_description.csv\")\n",
    "    data = pd.read_csv(\"C:/Users/samue/Downloads/dataset_description.csv\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(data['category'])\n",
    "\n",
    "    embedding_models = [\n",
    "        \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"all-MiniLM-L6-v2\"\n",
    "    ]\n",
    "\n",
    "    k_folds = [3, 5, 10] \n",
    "    seed = 42\n",
    "    results = {}\n",
    "\n",
    "    #Gerar Embbedings\n",
    "    input_base = \"C:/Users/samue/Downloads/NFT25/results_dynamic_2\"\n",
    "    #output_base = \"C:/Users/samue/Downloads/NFT25/results_dynamic\"\n",
    "    \n",
    "    #Carregar\n",
    "    #smote = BorderlineSMOTE(random_state=seed)\n",
    "    #output_base = \"C:/Users/samue/Downloads/NFT25/results_dynamic_BorderlineSMOTE\"\n",
    "\n",
    "    smote = SMOTE(random_state=seed)\n",
    "    output_base = \"C:/Users/samue/Downloads/NFT25/results_dynamic_SMOTE_new2\"\n",
    "\n",
    "    for (prep_name, prep_func), model_name in product(preprocess_methods.items(), embedding_models):\n",
    "\n",
    "        dir_name = f\"{prep_name}_{model_name.replace('/', '_')}\"\n",
    "        input_dir = os.path.join(input_base, dir_name)\n",
    "        output_dir = os.path.join(output_base, dir_name)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"‚ö†Ô∏èProcessando: {prep_name} | Modelo: {model_name}\")\n",
    "        processed_text = data['description'].fillna('').apply(prep_func)\n",
    "        \n",
    "        # Gerar Embbedings\n",
    "        #embeddings = get_text_features(processed_text, model_name)\n",
    "        #np.save(os.path.join(output_dir, 'embeddings.npy'), embeddings)\n",
    "        #print(f'‚úÖMatriz embeddings Salva! >>> {output_dir}')\n",
    "\n",
    "        # Carregar Embbedings\n",
    "        embeddings_path = os.path.join(input_dir, 'embeddings.npy')        \n",
    "        embeddings = np.load(embeddings_path)\n",
    "        print(f'‚úÖMatriz embeddings carregada! >>> {embeddings_path}')\n",
    "        print(f'‚úÖMatriz embeddings >>> {embeddings.shape}')\n",
    "        print(f'‚úÖMatriz embeddings >>> {embeddings}')\n",
    "        break\n",
    "\n",
    "\n",
    "        '''\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(embeddings)\n",
    "\n",
    "        for fold in k_folds:\n",
    "            results[str(fold) + '_folds'] = run_kmeans_cv(X_scaled, y, fold, seed, output_dir, label_encoder, smote)\n",
    "            print(f'| - {fold}_folds >>> accuracy: {results[str(fold) + \"_folds\"][\"overall_accuracy\"]}')\n",
    "\n",
    "        nome_arquivo = f'{output_dir}/metrics.json'\n",
    "        with open(nome_arquivo, 'w') as arquivo_json:\n",
    "            json.dump(results, arquivo_json, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"‚úÖDicion√°rio salvo em {nome_arquivo}\")'''\n",
    "        print(f\"‚úÖProcessamento conclu√≠do!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resultados organizados e salvos com sucesso em: C:/Users/samue/Downloads/NFT25/results_dynamic_SMOTE_new/results_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Listas dos tipos de pr√©-processamento e modelos\n",
    "preprocess_methods_ = [\n",
    "    'clean_stopwords_lemmatization', 'clean_stopwords', 'clean_lemmatization',\n",
    "    'stopwords_lemmatization', 'clean', 'stopwords', 'lemmatization', 'raw']\n",
    "    \n",
    "embedding_models = [\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"all-MiniLM-L6-v2\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for folder in os.listdir(output_base):\n",
    "    folder_path = os.path.join(output_base, folder)\n",
    "    metrics_file = os.path.join(folder_path, 'metrics.json')\n",
    "    \n",
    "    if os.path.isdir(folder_path) and os.path.exists(metrics_file):\n",
    "        preprocess = next((p for p in preprocess_methods_ if p in folder), 'Desconhecido')\n",
    "        model = next((m for m in embedding_models if m in folder), 'Desconhecido')\n",
    "        \n",
    "        with open(metrics_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for kfold, values in data.items():\n",
    "            if 'overall_accuracy' in values:\n",
    "                results.append({\n",
    "                    'modelos': model,\n",
    "                    'preprocessamento': preprocess,\n",
    "                    'kfolds': kfold,\n",
    "                    'acuracia': values['overall_accuracy']\n",
    "                })\n",
    "\n",
    "# Criar DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.sort_values(by=['modelos', 'preprocessamento', 'kfolds'], ascending=False, inplace=True)\n",
    "\n",
    "# Salvar o resultado em um arquivo CSV\n",
    "output_path = f\"{output_base}/results_summary.csv\"\n",
    "df_results.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Resultados organizados e salvos com sucesso em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os tr√™s arquivos CSV\n",
    "normal = pd.read_csv(\"C:/Users/samue/Downloads/NFT25/results_dynamic/results_summary.csv\")\n",
    "smote = pd.read_csv(\"C:/Users/samue/Downloads/NFT25/results_dynamic_SMOTE/results_summary.csv\")\n",
    "borderlinesMOTE = pd.read_csv(\"C:/Users/samue/Downloads/NFT25/results_dynamic_BorderlineSMOTE/results_summary.csv\")\n",
    "\n",
    "# Adicionar as colunas de acur√°cia para cada dataset\n",
    "normal['accBaseLine'] = normal['acuracia']\n",
    "smote['accSmote'] = smote['acuracia']\n",
    "borderlinesMOTE['accBorderlinesMOTE'] = borderlinesMOTE['acuracia']\n",
    "\n",
    "# Remover a coluna 'acuracia' original, pois j√° a incorporamos nas novas colunas\n",
    "normal.drop('acuracia', axis=1, inplace=True)\n",
    "smote.drop('acuracia', axis=1, inplace=True)\n",
    "borderlinesMOTE.drop('acuracia', axis=1, inplace=True)\n",
    "\n",
    "# Juntar os dataframes com base nas colunas 'modelos', 'preprocessamento', 'kfolds'\n",
    "merged_df = pd.merge(normal, smote, on=['modelos', 'preprocessamento', 'kfolds'], how='outer')\n",
    "merged_df = pd.merge(merged_df, borderlinesMOTE, on=['modelos', 'preprocessamento', 'kfolds'], how='outer')\n",
    "\n",
    "# Salvar o dataframe no formato CSV com separa√ß√£o por v√≠rgula\n",
    "output_path = \"C:/Users/samue/Downloads/NFT25/merged_results.csv\"\n",
    "merged_df.to_csv(output_path, index=False, decimal=',', sep='\\t')\n",
    "\n",
    "print(f\"‚úÖ Dataframe salvo em {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest ou SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
